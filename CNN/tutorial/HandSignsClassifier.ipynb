{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Refer from\n",
    "https://junstar92.tistory.com/105"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "def load_dataset():\n",
    "train_dataset = h5py.File('train_signs.h5', \"r\")\n",
    "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "test_dataset = h5py.File('test_signs.h5', \"r\")\n",
    "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "# Example of a picture\n",
    "index = 6\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[index])))\n",
    "X_train = X_train_orig.astype(np.float32)/255.\n",
    "X_test = X_test_orig.astype(np.float32)/255.\n",
    "Y_train = np.array(tf.one_hot(Y_train_orig, 6))\n",
    "Y_test = np.array(tf.one_hot(Y_test_orig, 6))\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "m = X.shape[0]\n",
    "mini_batches = []\n",
    "np.random.seed(seed)\n",
    "# Step 1: Shuffle (X, Y)\n",
    "permutation = list(np.random.permutation(m))\n",
    "shuffled_X = X[permutation,:,:,:]\n",
    "shuffled_Y = Y[permutation,:]\n",
    "# Step 2: Partition (shuffled_X, shuffled_Y)\n",
    "num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "for k in range(0, num_complete_minibatches):\n",
    "mini_batch_X = shuffled_X[k * mini_batch_size:k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "mini_batch_Y = shuffled_Y[k * mini_batch_size:k * mini_batch_size + mini_batch_size,:]\n",
    "mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "mini_batches.append(mini_batch)\n",
    "# Handling the end case\n",
    "if m % mini_batch_size != 0:\n",
    "mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:m,:,:,:]\n",
    "mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:m,:]\n",
    "mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "mini_batches.append(mini_batch)\n",
    "return mini_batchesb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(tf.keras.Model):\n",
    "def __init__(self):\n",
    "super(ConvNet, self).__init__()\n",
    "self.conv1 = tf.keras.layers.Conv2D(8, (4, 4), strides=(1,1), padding='same',\n",
    "                                    activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))\n",
    "self.maxpool1 = tf.keras.layers.MaxPool2D(pool_size=(8,8), strides=(8,8), padding='same')\n",
    "self.conv2 = tf.keras.layers.Conv2D(16, (2, 2), strides=(1,1), padding='same',\n",
    "                                    activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))\n",
    "self.maxpool2 = tf.keras.layers.MaxPool2D(pool_size=(4,4), strides=(4,4), padding='same')\n",
    "self.flatten = tf.keras.layers.Flatten()\n",
    "self.d1 = tf.keras.layers.Dense(6, activation='softmax',\n",
    "                                    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))\n",
    "def call(self, x):\n",
    "x = self.conv1(x)\n",
    "x = self.maxpool1(x)\n",
    "x = self.conv2(x)\n",
    "x = self.maxpool2(x)\n",
    "x = self.flatten(x)\n",
    "x = self.d1(x)\n",
    "return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def Model_1(X_train, Y_train, X_test, Y_test, learning_rate=0.009,\n",
    "    num_epochs=100, minibatch_size=64, print_cost=True):\n",
    "    tf.random.set_seed(4)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    seed = 3\n",
    "    costs = []\n",
    "    model = ConvNet()\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalCrossentropy(name='train_accuracy')\n",
    "    for epoch in range(num_epochs):\n",
    "        minibatch_cost=0\n",
    "        num_minibatches = int(m/minibatch_size)\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(minibatch_X)\n",
    "                cost = loss_object(minibatch_Y, pred)\n",
    "            grads = tape.gradient(cost, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            train_loss(cost)\n",
    "            train_accuracy(minibatch_Y, pred)\n",
    "            minibatch_cost += cost / num_minibatches\n",
    "        if print_cost and epoch % 5 == 0:\n",
    "            print(f'Cost after epoch {epoch}: {minibatch_cost}')\n",
    "        costs.append(minibatch_cost)\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    # training set\n",
    "    y_pred = model(X_train)\n",
    "    y_pred = tf.math.argmax(y_pred, 1)\n",
    "    y_true = tf.math.argmax(Y_train, 1)\n",
    "    train_acc = tf.math.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = tf.math.argmax(y_pred, 1)\n",
    "    y_true = tf.math.argmax(Y_test, 1)\n",
    "    test_acc = tf.math.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    print(f'Train acc : {train_acc}')\n",
    "    print(f'Test acc : {test_acc}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model_1(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_2(X_train, Y_train, X_test, Y_test, learning_rate=0.009,\n",
    "num_epochs=100, minibatch_size=64):\n",
    "    tf.random.set_seed(0)\n",
    "    seed = 3\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    costs = []\n",
    "    # initialize parameters\n",
    "    #parameters = initialize_parameters()\n",
    "    # setting optimizer\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    # CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(8, (4, 4), strides=(1,1), padding=\"SAME\",\n",
    "                                activation='relu', input_shape=(64, 64, 3),\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0)),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(8,8), strides=(8,8), padding=\"SAME\"),\n",
    "        tf.keras.layers.Conv2D(16, (2, 2), strides=(1,1), padding=\"SAME\", activation='relu',\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0)),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(4,4), strides=(4,4), padding=\"SAME\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='softmax',\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0))\n",
    "    ])\n",
    "    model.summary()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    hist = model.fit(X_train, Y_train, epochs=num_epochs, batch_size=minibatch_size, verbose=1)\n",
    "    costs = hist.history['loss']\n",
    "    for epoch in range(0, num_epochs, 5):\n",
    "        print(f'Cost after epoch {epoch}: {costs[epoch]}')\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    train_acc = hist.history['accuracy'][-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = tf.math.argmax(y_pred, 1)\n",
    "    y_true = tf.math.argmax(Y_test, 1)\n",
    "    test_acc = tf.math.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    print(f'Train acc : {train_acc}')\n",
    "    print(f'Test acc : {test_acc}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Model_2(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_2_Reg(X_train, Y_train, X_test, Y_test, learning_rate=0.009,\n",
    "num_epochs=100, minibatch_size=64, lambd=0.01):\n",
    "    tf.random.set_seed(2)\n",
    "    seed = 3\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    costs = []\n",
    "    # initialize parameters\n",
    "    #parameters = initialize_parameters()\n",
    "    # setting optimizer\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    # setting regularizer\n",
    "    regularizer = tf.keras.regularizers.L2(lambd)\n",
    "    # CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(8, (4, 4), strides=(1,1), padding=\"SAME\",\n",
    "                                activation='relu', input_shape=(64, 64, 3),\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0),\n",
    "                                kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(8,8), strides=(8,8), padding=\"SAME\"),\n",
    "        tf.keras.layers.Conv2D(16, (2, 2), strides=(1,1), padding=\"SAME\", activation='relu',\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0),\n",
    "                                kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(4,4), strides=(4,4), padding=\"SAME\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='softmax',\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=0),\n",
    "                                kernel_regularizer=regularizer)\n",
    "    ])\n",
    "    model.summary()\n",
    "    model.compile(optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "    hist = model.fit(X_train, Y_train, epochs=num_epochs, batch_size=minibatch_size, verbose=1)\n",
    "    costs = hist.history['loss']\n",
    "    for epoch in range(0, num_epochs, 5):\n",
    "        print(f'Cost after epoch {epoch}: {costs[epoch]}')\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    train_acc = hist.history['accuracy'][-1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = tf.math.argmax(y_pred, 1)\n",
    "    y_true = tf.math.argmax(Y_test, 1)\n",
    "    test_acc = tf.math.reduce_mean(tf.cast(tf.equal(y_pred, y_true), tf.float32))\n",
    "    print(f'Train acc : {train_acc}')\n",
    "    print(f'Test acc : {test_acc}')\n",
    "    return model"
   ]
  }
 ]
}